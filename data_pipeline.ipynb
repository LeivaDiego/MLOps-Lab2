{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd3d442",
   "metadata": {},
   "source": [
    "# Laboratorio 2: Data Understanding\n",
    "\n",
    "**Universidad del Valle de Guatemala**  \n",
    "**Facultad de Ingeniería**  \n",
    "**Departamento de Ciencias de la Computación**  \n",
    "**Machine Learning Operations** \n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Arturo Argueta - 21527 \n",
    "- Edwin de León - 22809 \n",
    "- Diego Leiva - 21752 \n",
    "- Pablo Orellana - 21970"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bd57d",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a8a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac025c5a",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51ac29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Change to DEBUG for more detailed output\n",
    "    format=\"[%(levelname)s] - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f3b86",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157e5bd",
   "metadata": {},
   "source": [
    "Durante el análisis exploratorio se encontró que hay conjuntos de datos con codificaciones diferentes a la típica utf-8, por lo que se necesita una lectura segura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1896e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_with_fallback(\n",
    "        path: Path, \n",
    "        encodings: Tuple[str, ...] = (\"utf-8\", \"utf-8-sig\", \"latin1\", \"cp1252\"), \n",
    "        **pd_kwargs\n",
    "    ) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Attempts to read a CSV file with various encodings to find the correct one.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file.\n",
    "        encodings (tuple): Encodings to try.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, str]: The DataFrame read and the encoding used.\n",
    "    \"\"\"\n",
    "    last_error = None\n",
    "\n",
    "    # Iterating over encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # Try reading the CSV with the current encoding\n",
    "            df = pd.read_csv(path, encoding=encoding, **pd_kwargs)\n",
    "            # Standardize column names\n",
    "            df.columns = (df.columns\n",
    "                          .str.strip() # Remove leading and trailing whitespace\n",
    "                          .str.replace(r\"\\s+\", \" \", regex=True) # Replace multiple spaces with a single space\n",
    "                         )\n",
    "            \n",
    "            # If successful, return the DataFrame and the encoding used\n",
    "            return df, encoding\n",
    "\n",
    "        # If it fails, try the next encoding\n",
    "        except UnicodeDecodeError as e:\n",
    "            last_error = e\n",
    "\n",
    "        # If it fails, capture the error\n",
    "        except Exception as e:\n",
    "            # Other errors; keep track and keep trying in case it's just encoding\n",
    "            last_error = e\n",
    "\n",
    "    # Last \"tolerant\" attempt: utf-8 with replacement for bad characters\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            # Try reading the CSV with the current encoding\n",
    "            df = pd.read_csv(f, **pd_kwargs)\n",
    "            # Standardize column names\n",
    "            df.columns = df.columns.str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            # If successful, return the DataFrame and the encoding used\n",
    "            return df, \"utf-8(errors=replace)\"\n",
    "    \n",
    "    # If not successful, keep track of the last error\n",
    "    except Exception:\n",
    "        raise last_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad68faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csvs(\n",
    "        folder: str,\n",
    "        pattern: str = \"*.csv\",\n",
    "        encodings: Tuple[str, ...] = (\"utf-8\", \"utf-8-sig\", \"latin1\", \"cp1252\"),\n",
    "        **pd_kwargs\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load CSV files from a folder into a dictionary of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        folder (str): The folder containing the CSV files.\n",
    "        pattern (str, optional): The glob pattern to match CSV files. Defaults to \"*.csv\".\n",
    "        encodings (Tuple[str, ...], optional): The encodings to try when reading CSV files. Defaults to (\"utf-8\", \"latin1\", \"cp1252\").\n",
    "        **pd_kwargs: Additional keyword arguments to pass to pandas read_csv.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: A dictionary mapping file names (without extensions) to DataFrames.\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the folder\n",
    "    files = sorted(Path(folder).glob(pattern))\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # Read each CSV file into a DataFrame\n",
    "    for file in tqdm(files, desc=\"Reading CSVs\", unit=\"file\"):\n",
    "        df, encoding = read_csv_with_fallback(file, encodings=encodings, **pd_kwargs)\n",
    "        name = file.stem.lower()  # Use the file name without extension as the key\n",
    "        dfs[name] = df # Store the DataFrame with the file name (without extension) as the key\n",
    "        logger.debug(f\"Loaded {name} with encoding {encoding}\")\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e083a",
   "metadata": {},
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644b6a7",
   "metadata": {},
   "source": [
    "### Normalización de Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e8041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_strings(\n",
    "        df: pd.DataFrame, \n",
    "        cols: list[str]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize string columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        cols (list[str]): The columns to normalize.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with normalized string columns.\n",
    "    \"\"\"\n",
    "    out = df.copy() # Create a copy of the DataFrame\n",
    "    # Iterate over the specified columns\n",
    "    for c in cols:\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if c in out.columns:\n",
    "            # Normalize the string values in the column\n",
    "            out[c] = (out[c].astype(\"string\")\n",
    "                      .str.strip() # Remove leading and trailing whitespace\n",
    "                      .str.replace(r\"\\s+\", \" \", regex=True) # Replace multiple spaces with a single space\n",
    "                     )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103d1cef",
   "metadata": {},
   "source": [
    "### Coerción de tipos en llaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ffd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coerce_column_types(\n",
    "        dfs: Dict[str, pd.DataFrame], \n",
    "        schema: Dict[str, Dict[str, str]]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Coerce DataFrame column types according to a schema.\n",
    "\n",
    "    Args:\n",
    "        dfs (Dict[str, pd.DataFrame]): The input DataFrames.\n",
    "        schema (Dict[str, Dict[str, str]]): The schema defining the desired column types.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: The DataFrames with coerced column types.\n",
    "    \"\"\"\n",
    "    out: Dict[str, pd.DataFrame] = dfs.copy() # Create output dictionary\n",
    "\n",
    "    # Iterate over the input DataFrames\n",
    "    for name, df in dfs.items():\n",
    "        dfo = df.copy() # Create a copy of the DataFrame\n",
    "\n",
    "        # Coerce types according to the schema\n",
    "        if name in schema:\n",
    "            # Iterate over the columns and their desired types\n",
    "            for col, dtype in schema[name].items():\n",
    "                if col in dfo.columns:\n",
    "                    # Coerce the column to the desired type\n",
    "                    dfo[col] = pd.to_numeric(dfo[col], errors=\"coerce\") if dtype in (\"Int64\",\"Float64\",\"int\",\"float\") else dfo[col].astype(dtype)\n",
    "        out[name] = dfo\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba273",
   "metadata": {},
   "source": [
    "### Manejo de duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a136d58",
   "metadata": {},
   "source": [
    "Durante el análisis exploratorio se encontró que los conjuntos de datos de clientes y eventos tienen registros completos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac69d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_handling(\n",
    "        df: pd.DataFrame,\n",
    "        subset: list[str] | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle duplicates in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        subset (list[str] | None): The columns to consider for identifying duplicates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame without duplicates.\n",
    "    \"\"\"\n",
    "    # Drop duplicates based on the subset of columns\n",
    "    df_unique = df.drop_duplicates(subset=subset, keep=\"first\").copy()\n",
    "    # Reset the index\n",
    "    df_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087df1f",
   "metadata": {},
   "source": [
    "### Manejo de nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92e85c",
   "metadata": {},
   "source": [
    "Durante el análisis exploratorio se encontró que el conjunto de datos de  `clientes` tiene 281 registros nulos en cada columna (2.34%). en `eventos` solo la variable `transactionid` tiene 99.9% de valores nulos, y para `producto` las variables `categoria_id` y `marca_id` tienen 8.55% y 7.28% respectivamente y `precio` tiene un 0.05% de nulos.\n",
    "\n",
    "Para tratarlos se decide eliminar los resgistros con datos nulos para `clientes` y la variable `precio`, para las variables `categoria_id` y `marca_id` se decide mapearlos como desconocido u otros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17039506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_free_id(\n",
    "        series: pd.Series\n",
    "    ) -> int:\n",
    "    \"\"\"\n",
    "    Get the next free (unused) ID from a series of existing IDs.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): Series of existing IDs.\n",
    "\n",
    "    Returns:\n",
    "        int: Next free ID.\n",
    "    \"\"\"\n",
    "    # Get the next free ID\n",
    "    if series.empty:\n",
    "        # If the series is empty, return 1 as the next ID\n",
    "        return 1\n",
    "    \n",
    "    return int(pd.to_numeric(series, errors=\"coerce\").max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96ecbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_handling(\n",
    "        dfs: Dict[str, pd.DataFrame]\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Handle null values in the given DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        dfs (Dict[str, pd.DataFrame]): The DataFrames to process.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: The processed DataFrames.\n",
    "    \"\"\"\n",
    "    out: Dict[str, pd.DataFrame] = dfs.copy() # Initialize output dictionary\n",
    "\n",
    "    # --- Handle CATEGORIA ---\n",
    "    category_df = dfs[\"categoria\"].copy() # create a copy of the categoria DataFrame\n",
    "    # Check if 'Otro' category exists\n",
    "    if not (category_df[\"categoria\"] == \"Otro\").any():\n",
    "        # If not, create it\n",
    "        new_category_id = next_free_id(category_df[\"id\"]) # Get the next free ID\n",
    "        # Append the new category with the next free ID and the name 'Otro'\n",
    "        category_df = pd.concat(\n",
    "            [category_df, pd.DataFrame([{\"id\": new_category_id, \"categoria\": \"Otro\"}])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    out[\"categoria\"] = category_df\n",
    "\n",
    "    # --- Handle MARCA ---\n",
    "    brand_df = dfs[\"marca\"].copy() # create a copy of the marca DataFrame\n",
    "    # Check if 'Otro' brand exists\n",
    "    if not (brand_df[\"marca\"] == \"Otro\").any():\n",
    "        # If not, create it\n",
    "        new_brand_id = next_free_id(brand_df[\"id\"]) # Get the next free ID\n",
    "        # Append the new brand with the next free ID and the name 'Otro'\n",
    "        brand_df = pd.concat(\n",
    "            [brand_df, pd.DataFrame([{\"id\": new_brand_id, \"marca\": \"Otro\"}])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    out[\"marca\"] = brand_df\n",
    "\n",
    "    # Get IDs for 'Otro' category and brand\n",
    "    unknown_category_id = int(out[\"categoria\"].loc[out[\"categoria\"][\"categoria\"] == \"Otro\", \"id\"].iloc[0])\n",
    "    unknown_brand_id = int(out[\"marca\"].loc[out[\"marca\"][\"marca\"] == \"Otro\", \"id\"].iloc[0])\n",
    "\n",
    "    # --- Handle PRODUCTO ---\n",
    "    product_df = dfs[\"producto\"].copy()\n",
    "    # Drop any rows with null values on precio\n",
    "    product_df = product_df.dropna(subset=[\"precio\"])\n",
    "    # Impute null foreign keys as 'Otro'\n",
    "    # Fill null values in 'categoria_id' with the ID of 'Otro' category\n",
    "    product_df[\"categoria_id\"] = product_df[\"categoria_id\"].fillna(unknown_category_id)\n",
    "    # Fill null values in 'marca_id' with the ID of 'Otro' brand\n",
    "    product_df[\"marca_id\"] = product_df[\"marca_id\"].fillna(unknown_brand_id)\n",
    "    out[\"producto\"] = product_df\n",
    "\n",
    "    # --- Handle CLIENTE ---\n",
    "    client_df = dfs[\"cliente\"].copy()\n",
    "    # Drop any rows with null values as primary keys (id)\n",
    "    client_df = client_df.dropna(subset=[\"id\"]).copy()\n",
    "    # Ensure all string columns are stripped of whitespace\n",
    "    for col in [\"nombre\", \"apellido\", \"genero\", \"empresa\", \"idioma\", \"nit\", \"puesto\", \"ciudad\", \"correo\", \"telefono\"]:\n",
    "        if col in client_df.columns:\n",
    "            client_df[col] = client_df[col].astype(\"string\").str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    \n",
    "    # --- Handle EVENTO ---\n",
    "    event_df = dfs[\"evento\"].copy() # create a copy of the evento DataFrame\n",
    "    # Check if 'transactionid' column exists\n",
    "    if \"transactionid\" in event_df.columns:\n",
    "        # Drop the 'transactionid' column\n",
    "        event_df = event_df.drop(columns=[\"transactionid\"])\n",
    "    out[\"evento\"] = event_df\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3659f3",
   "metadata": {},
   "source": [
    "### Filtro de productos válidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d86679",
   "metadata": {},
   "source": [
    "Se aplica un filtro obligatorio para conservar únicamente aquellos eventos cuyo `itemid` esté presente y referenciado en `producto.id`. De esta forma se eliminan todos los registros donde el producto del evento es desconocido, garantizando la integridad referencial con la tabla de productos y permitiendo calcular métricas consistentes de ventas, categorías y marcas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ace54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events_by_valid_product(\n",
    "        dfs: Dict[str, pd.DataFrame],\n",
    "        itemid_col: str = \"itemid\",\n",
    "        prodid_col: str = \"id\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter events by valid product IDs.\n",
    "\n",
    "    Args:\n",
    "        dataframes (Dict[str, pd.DataFrame]): Input DataFrames containing events and products.\n",
    "        itemid_col (str): Name of the item ID column in the event DataFrame.\n",
    "        prodid_col (str): Name of the product ID column in the product DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of filtered events.\n",
    "    \"\"\"\n",
    "    # Create copies of event and product DataFrames\n",
    "    events = dfs[\"evento\"].copy()\n",
    "    products = dfs[\"producto\"].copy()\n",
    "\n",
    "    # Check columns of keys are not missin\n",
    "    if itemid_col not in events.columns or prodid_col not in products.columns:\n",
    "        logger.warning(f\"Columns {itemid_col} or {prodid_col} not found in DataFrames.\")\n",
    "        return events\n",
    "\n",
    "    # Get valid product IDs\n",
    "    prod_ids = set(products[prodid_col].dropna().unique())\n",
    "\n",
    "    # Generate mask for valid events\n",
    "    mask_valid = events[itemid_col].isin(prod_ids)\n",
    "    total, valid = len(events), int(mask_valid.sum())\n",
    "    invalid = total - valid\n",
    "\n",
    "    logger.debug(f\"Filtering events: {valid} valid, {invalid} invalid out of {total} total events.\")\n",
    "\n",
    "    # Filter events by valid product IDs\n",
    "    valid_events = events.loc[mask_valid].copy()\n",
    "\n",
    "    # Sanity check: all item IDs in valid events must be in product IDs\n",
    "    assert valid_events[itemid_col].dropna().isin(prod_ids).all(), \\\n",
    "        \"Mismatched item IDs found after filtering.\"\n",
    "\n",
    "    return valid_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f519edf",
   "metadata": {},
   "source": [
    "### Manejo de fechas y timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd711924",
   "metadata": {},
   "source": [
    "#### Conversión de timestamps a fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67db957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(\n",
    "        df: pd.DataFrame, \n",
    "        tmstp_col: str = \"timestamp\", \n",
    "        date_col: str = \"fecha\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert timestamp (seconds or milliseconds) column to datetime.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to process.\n",
    "        tmstp_col (str): Name of the timestamp column.\n",
    "        date_col (str): Name of the resulting date column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the converted date column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Check if timestamp column exists\n",
    "    if tmstp_col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    # Determine the unit of the timestamp\n",
    "    # Heuristic: If the value is less than 1e11, it's in seconds, otherwise milliseconds\n",
    "    sample_val = df[tmstp_col].dropna().iloc[0] # Get a sample value from the timestamp column\n",
    "    unit = 's' if sample_val < 1e11 else 'ms' # Determine the unit of the timestamp\n",
    "\n",
    "    # Convert the timestamp column to datetime\n",
    "    df[date_col] = pd.to_datetime(df[tmstp_col], unit=unit, errors=\"coerce\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb7929",
   "metadata": {},
   "source": [
    "#### Limpieza y formateo de fecha de nacimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7772436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_birthdate_and_age(\n",
    "        dfs: Dict[str, pd.DataFrame], \n",
    "        client_col: str = \"cliente\", \n",
    "        birth_col: str = \"nacimiento\", \n",
    "        age_col: str = \"edad\"\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Clean birthdate and age fields in the specified DataFrame.\n",
    "    Handle parsing, future date correction, and age recalculation.\n",
    "\n",
    "    Args:\n",
    "        dataframes (Dict[str, pd.DataFrame]): Dictionary of DataFrames.\n",
    "        client_col (str): Key for the DataFrame to process.\n",
    "        birth_col (str): Name of the birthdate column.\n",
    "        age_col (str): Name of the age column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Updated dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    # Check if client exists\n",
    "    if client_col not in dfs:\n",
    "        return dfs\n",
    "    \n",
    "    df = dfs[client_col].copy() # Create a copy of the DataFrame\n",
    "\n",
    "    # Parse date field to datetime format\n",
    "    dob = pd.to_datetime(df[birth_col], format=\"%m/%d/%y\", errors=\"coerce\")\n",
    "\n",
    "    # Fallback\n",
    "    mask_fallback = dob.isna() & df[birth_col].notna()\n",
    "    if mask_fallback.any():\n",
    "        dob_alt = pd.to_datetime(df.loc[mask_fallback, birth_col], errors=\"coerce\")\n",
    "        dob.loc[mask_fallback] = dob_alt\n",
    "\n",
    "    # Correct future years\n",
    "    today = pd.Timestamp.today().normalize() # Get today's date\n",
    "    future_date_mask = dob > today # Identify future dates\n",
    "    dob.loc[future_date_mask] -= pd.DateOffset(years=100) # Subtract 100 years from future dates\n",
    "\n",
    "    # Compute age\n",
    "    age = ((today - dob).dt.days / 365.25).round().astype(\"Int64\")\n",
    "\n",
    "    # Reassign age to the DataFrame\n",
    "    df[birth_col] = dob\n",
    "    df[age_col] = age\n",
    "\n",
    "    dfs[client_col] = df # Update the DataFrame in the dictionary\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e5055",
   "metadata": {},
   "source": [
    "## Transformación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc20c33",
   "metadata": {},
   "source": [
    "### Mapeo de clientes desconocidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d689ae",
   "metadata": {},
   "source": [
    "Se crea una nueva entrada en la tabla clientes para agregar a cliente anónimo con datos vacios, esto para luego sustituir todos los ids de clientes desconocidos por un único cliente desconocido referenciado en la tabla clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93fa74c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_anonymous_client(\n",
    "        client_df: pd.DataFrame,\n",
    "        anon_name: str = \"Anonimo\",\n",
    "        anon_gender: str = \"NA\"\n",
    "    ) -> tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Ensures that an anonymous client exists in the DataFrame.\n",
    "    A new anonymous client is created if one does not already exist.\n",
    "\n",
    "    Args:\n",
    "        client_df (pd.DataFrame): The DataFrame containing client information.\n",
    "        anon_name (str): The name to use for the anonymous client.\n",
    "        anon_gender (str): The gender to use for the anonymous client.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, int]: The updated DataFrame and the ID of the anonymous client.\n",
    "    \"\"\"\n",
    "    df = client_df.copy() # Create a copy of the client DataFrame\n",
    "\n",
    "    # Check if both nombre and apellido exist\n",
    "    if {\"nombre\", \"apellido\"}.issubset(df.columns):\n",
    "        # Generate a mask for existing anonymous client\n",
    "        mask_existing = (df[\"nombre\"].fillna(\"\").str.casefold() == anon_name.casefold()) & (df[\"apellido\"].fillna(\"\") == \"\")\n",
    "        if mask_existing.any():\n",
    "            # If an anonymous client already exists, return the original DataFrame and its ID\n",
    "            anon_id = int(df.loc[mask_existing, \"id\"].iloc[0])\n",
    "            return df, anon_id\n",
    "\n",
    "    # If no anonymous client exists, create a new one\n",
    "    anon_id = next_free_id(pd.to_numeric(df[\"id\"], errors=\"coerce\")) # Generate a new ID\n",
    "\n",
    "    # Define the anonymous client entry\n",
    "    row = {c: pd.NA for c in df.columns} # Initialize all columns with NA\n",
    "    row.update({\n",
    "        \"id\": anon_id,\n",
    "        \"nombre\": anon_name,\n",
    "        \"apellido\": \"\",\n",
    "        \"genero\": anon_gender\n",
    "    })\n",
    "    row_df = pd.DataFrame([row]).astype(df.dtypes.to_dict(), errors=\"ignore\") # Create a DataFrame for the new row\n",
    "    df = pd.concat([df, row_df], ignore_index=True)\n",
    "\n",
    "    return df, anon_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c85c2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_unknown_visitors_to_anonymous(\n",
    "        dfs: Dict[str, pd.DataFrame],\n",
    "        visitor_col: str = \"visitorid\",\n",
    "        client_col: str = \"id\",\n",
    "        anon_id: int = None,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map unknown visitors to an anonymous client in the event DataFrame.\n",
    "    Args:\n",
    "        dfs (Dict[str, pd.DataFrame]): Input DataFrames containing events and clients.\n",
    "        visitor_col (str): Name of the visitor ID column in the event DataFrame.\n",
    "        client_col (str): Name of the client ID column in the client DataFrame.\n",
    "        anon_id (int): ID of the anonymous client.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of events with unknown visitors mapped to the anonymous client.\n",
    "    \"\"\"\n",
    "    mapped_events = dfs[\"evento\"].copy() # Create a copy of the event DataFrame\n",
    "    clients = dfs[\"cliente\"].copy() # Create a copy of the client DataFrame\n",
    "\n",
    "    # Set of valid client IDs\n",
    "    valid_ids = set(pd.to_numeric(clients[client_col], errors=\"coerce\").dropna().unique())\n",
    "\n",
    "    # Add known flag dimension\n",
    "    mapped_events[\"cliente_conocido\"] = mapped_events[visitor_col].isin(valid_ids)\n",
    "\n",
    "    # Mask of unknowns, NaN or not in clients features\n",
    "    mask_unknown = (~mapped_events[visitor_col].isin(valid_ids)) | (mapped_events[visitor_col].isna())\n",
    "\n",
    "    if anon_id is None:\n",
    "        # Ensure an anonymous client exists\n",
    "        raise ValueError(\"Anonymous client ID must be provided or created.\")\n",
    "\n",
    "    # Reassign unknown visitors to the anonymous client\n",
    "    mapped_events.loc[mask_unknown, visitor_col] = anon_id\n",
    "\n",
    "    return mapped_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac56239",
   "metadata": {},
   "source": [
    "### Creación de dimensiones adicionales en tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46606136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dim_cliente(\n",
    "        client_df: pd.DataFrame,\n",
    "        full_name_col: str = \"cliente\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a dimension for clients with a full name column.\n",
    "    A full name is created by merging the \"nombre\" and \"apellido\" columns.\n",
    "\n",
    "    Args:\n",
    "        client_df (pd.DataFrame): The DataFrame containing client information.\n",
    "        full_name_col (str): The name of the full name column to create.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with the full name column.\n",
    "    \"\"\"\n",
    "    base_cols = client_df.columns.tolist() # Get all columns from the client DataFrame\n",
    "    base_cols.append(\"edad\") # Add age column if it exists\n",
    "\n",
    "    # Merge full name\n",
    "    client_df[full_name_col] = (\n",
    "        client_df.get(\"nombre\", \"\").fillna(\"\") + \" \" +\n",
    "        client_df.get(\"apellido\", \"\").fillna(\"\")\n",
    "        ).str.strip()\n",
    "    \n",
    "    return client_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d8287",
   "metadata": {},
   "source": [
    "### Selección de Dimensiones de las tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fb75c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(\n",
    "        df: pd.DataFrame,\n",
    "        cols: list[str]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a copy of the DataFrame with only the specified columns, in the given order,\n",
    "    silently ignoring any that do not exist.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        cols (list[str]): The columns to select.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the DataFrame with only the selected columns.\n",
    "    \"\"\"\n",
    "    # Select only the columns that exist in the DataFrame\n",
    "    keep = [c for c in cols if c in df.columns]\n",
    "    selected = df[keep].copy()\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe8173",
   "metadata": {},
   "source": [
    "### Renombramiento de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca1cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(\n",
    "        df: pd.DataFrame, \n",
    "        mapping: dict[str, str]\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Renames columns according to mapping {original: new}.\n",
    "    Does not fail if any key does not exist.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        mapping (dict[str, str]): A dictionary mapping original column names to new names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    # Rename columns according to mapping\n",
    "    renamed_df = df.rename(columns={k: v for k, v in mapping.items() if k in df.columns}).copy()\n",
    "    return renamed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4f2e8",
   "metadata": {},
   "source": [
    "## Exportacion de tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d18fa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tables(\n",
    "        dfs: Dict[str, pd.DataFrame],\n",
    "        out_folder: str = \"data/processed\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Export multiple DataFrames to CSV files.\n",
    "\n",
    "    Args:\n",
    "        dfs (Dict[str, pd.DataFrame]): The DataFrames to export.\n",
    "        names (list[str]): The names of the output CSV files.\n",
    "    \"\"\"\n",
    "    if out_folder is not None:\n",
    "        out_path = Path(out_folder)\n",
    "        out_path.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "        \n",
    "        # Define a tqdm iterator\n",
    "        iterator = tqdm(dfs.items(), total=len(dfs), desc=\"Exporting CSVs\", unit=\"table\")\n",
    "\n",
    "        # Iterate over dataframes\n",
    "        for name, df in iterator:\n",
    "            path = out_path / f\"{name}.csv\"\n",
    "            try:\n",
    "                df.to_csv(path, index=False)\n",
    "                iterator.set_postfix(rows=len(df), file=path.name, refresh=False)\n",
    "            except Exception as e:\n",
    "                msg = f\"Failed to export {name} to {path}: {e}\"\n",
    "                tqdm.write(msg)\n",
    "                logger.exception(msg)\n",
    "            else:\n",
    "                logger.debug(f\"Exported {name} to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a177ea39",
   "metadata": {},
   "source": [
    "## Integración de Pipeline completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b48f0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "        raw_path: str = \"data/raw\"\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    logger.info(\"Starting data processing pipeline...\")\n",
    "    # 0) Load raw data\n",
    "    logger.debug(\"Loading raw data...\")\n",
    "    dfs = load_csvs(folder=raw_path)\n",
    "    logger.debug(\"Raw data loaded successfully\")\n",
    "\n",
    "    # Sanity Check: Validate all tables are present\n",
    "    required = {\"categoria\",\"cliente\",\"evento\",\"marca\",\"producto\"}\n",
    "    missing = required - set(dfs.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"Faltan CSVs requeridos: {missing}\")\n",
    "\n",
    "    # ----- Data Cleaning -----\n",
    "    logger.info(\"Data cleaning stage initialized\")\n",
    "\n",
    "    # 1) Handle duplicates on each table\n",
    "    logger.debug(\"Handling duplicates...\")\n",
    "    dfs[\"categoria\"] = duplicates_handling(dfs[\"categoria\"], subset=None)\n",
    "    dfs[\"marca\"] = duplicates_handling(dfs[\"marca\"], subset=None)\n",
    "    dfs[\"producto\"] = duplicates_handling(dfs[\"producto\"], subset=[\"id\"])\n",
    "    dfs[\"cliente\"] = duplicates_handling(dfs[\"cliente\"], subset=[\"id\"])\n",
    "    dfs[\"evento\"] = duplicates_handling(dfs[\"evento\"], subset=None)\n",
    "    logger.debug(\"Duplicates handled successfully\")\n",
    "\n",
    "    # 2) String normalization\n",
    "    logger.debug(\"Normalizing strings...\")\n",
    "    dfs[\"categoria\"] = normalize_strings(dfs[\"categoria\"], [\"categoria\"])\n",
    "    dfs[\"marca\"] = normalize_strings(dfs[\"marca\"], [\"marca\"])\n",
    "    dfs[\"producto\"] = normalize_strings(dfs[\"producto\"], [\"nombre\"])\n",
    "    dfs[\"cliente\"] = normalize_strings(\n",
    "            dfs[\"cliente\"],\n",
    "            [\"nombre\",\"apellido\",\"genero\",\"empresa\",\"idioma\",\n",
    "            \"nit\",\"puesto\",\"ciudad\",\"correo\",\"telefono\"]\n",
    "        )\n",
    "    logger.debug(\"Strings normalized successfully\")\n",
    "\n",
    "    # 3) Data typing by schema\n",
    "    schema = {\n",
    "        \"categoria\": {\"id\":\"Int64\"},\n",
    "        \"marca\":     {\"id\":\"Int64\"},\n",
    "        \"producto\":  {\"id\":\"Int64\",\"categoria_id\":\"Int64\",\n",
    "                      \"marca_id\":\"Int64\",\"volumen\":\"Int64\",\"precio\":\"Float64\"},\n",
    "        \"cliente\":   {\"id\":\"Int64\"},\n",
    "        \"evento\":    {\"timestamp\":\"Int64\",\"visitorid\":\"Int64\",\"itemid\":\"Int64\"}\n",
    "    }\n",
    "    logger.debug(\"Coercing column types...\")\n",
    "    dfs = coerce_column_types(dfs, schema)\n",
    "    logger.debug(\"Column types coerced successfully\")\n",
    "\n",
    "    # 4) Handle Null values\n",
    "    logger.debug(\"Handling null values...\")\n",
    "    dfs = nulls_handling(dfs)\n",
    "    logger.debug(\"Null values handled successfully\")\n",
    "\n",
    "    # 5) Filter event register by valid product IDs\n",
    "    eventos_valid = filter_events_by_valid_product(dfs, itemid_col=\"itemid\", prodid_col=\"id\")\n",
    "    dfs[\"evento\"] = eventos_valid\n",
    "\n",
    "    # 6) Convert timestamps to datetime data\n",
    "    logger.debug(\"Converting timestamps to datetime in events table...\")\n",
    "    dfs[\"evento\"] = convert_timestamp(dfs[\"evento\"], tmstp_col=\"timestamp\", date_col=\"fecha\")\n",
    "    logger.debug(\"Timestamps converted successfully\")\n",
    "\n",
    "    # 7) Handle birth date format and compute client age\n",
    "    logger.debug(\"Cleaning birthdate and computing age in clients table...\")\n",
    "    dfs = clean_birthdate_and_age(dfs, client_col=\"cliente\", birth_col=\"nacimiento\", age_col=\"edad\")\n",
    "    logger.debug(\"Birthdate cleaned and age computed successfully\")\n",
    "\n",
    "    # ----- Data transformation -----\n",
    "    logger.info(\"Data transformation stage initialized...\")\n",
    "\n",
    "    # 8) Anonymous clients mapping\n",
    "    logger.debug(\"Mapping unknown visitors id to anonymoys client...\")\n",
    "    dfs[\"cliente\"], anon_id = ensure_anonymous_client(dfs[\"cliente\"], anon_name=\"Anonimo\", anon_gender=\"NA\")\n",
    "    dfs[\"evento\"] = map_unknown_visitors_to_anonymous(\n",
    "        dfs, visitor_col=\"visitorid\", client_col=\"id\", anon_id=anon_id\n",
    "    )\n",
    "    logger.debug(\"Unknown visitors mapped to anonymous client successfully\")\n",
    "\n",
    "    # 9) Create new tables dimensions and column renaming\n",
    "    logger.debug(\"Creating new dimensions and renaming columns...\")\n",
    "    # Select relevant product dimensions\n",
    "    producto  = select_columns(dfs[\"producto\"], [\"id\",\"categoria_id\",\"nombre\",\"marca_id\",\"volumen\",\"precio\"])\n",
    "    producto  = rename_columns(producto, {\"nombre\":\"producto\"})\n",
    "    # Select relevant category dimensions\n",
    "    categoria = select_columns(dfs[\"categoria\"], [\"id\",\"categoria\"])\n",
    "    # Select relevant brand dimensions\n",
    "    marca     = select_columns(dfs[\"marca\"],     [\"id\",\"marca\"])\n",
    "    # Build new client full name dimension and select relevant dimensions\n",
    "    cliente_full = build_dim_cliente(dfs[\"cliente\"], full_name_col=\"cliente\")\n",
    "    cliente      = select_columns(cliente_full, [\"id\",\"cliente\",\"genero\",\"edad\"])\n",
    "    \n",
    "    # 10) Create a cleaned evento table\n",
    "    evento = select_columns(dfs[\"evento\"], [\"event\",\"fecha\",\"visitorid\",\"itemid\",\"cliente_conocido\"])\n",
    "    evento = rename_columns(evento, \n",
    "                                 {\"event\":\"evento\", \n",
    "                                  \"visitorid\":\"cliente_id\", \n",
    "                                  \"itemid\":\"producto_id\"})\n",
    "    \n",
    "    # 11) Required typing\n",
    "    evento[\"cliente_id\"]  = pd.to_numeric(evento[\"cliente_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    evento[\"producto_id\"] = pd.to_numeric(evento[\"producto_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    cliente[\"id\"] = pd.to_numeric(cliente[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    producto[\"categoria_id\"] = pd.to_numeric(producto[\"categoria_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    producto[\"marca_id\"] = pd.to_numeric(producto[\"marca_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    logger.debug(\"New dimensions created and columns renamed successfully\")\n",
    "\n",
    "    # 11) Export tables as csv files\n",
    "    logger.debug(\"Exporting tables to CSV files...\")\n",
    "    outputs = {\n",
    "        \"fact_evento\":   evento,\n",
    "        \"dim_producto\":  producto,\n",
    "        \"dim_categoria\": categoria,\n",
    "        \"dim_marca\":     marca,\n",
    "        \"dim_cliente\":   cliente\n",
    "    }\n",
    "    export_tables(outputs, out_folder=\"data/processed\")\n",
    "    logger.debug(\"Tables exported successfully\")\n",
    "    logger.info(\"Data pipeline completed successfully\")\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e893756e",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a63937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] - Starting data processing pipeline...\n",
      "Reading CSVs: 100%|██████████| 5/5 [00:00<00:00,  7.04file/s]\n",
      "[INFO] - Data cleaning stage initialized\n",
      "[INFO] - Data transformation stage initialized...\n",
      "Exporting CSVs: 100%|██████████| 5/5 [00:01<00:00,  2.63table/s, file=dim_cliente.csv, rows=11721] \n",
      "[INFO] - Data pipeline completed successfully\n"
     ]
    }
   ],
   "source": [
    "dfs = run_pipeline(raw_path=\"data/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1316af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fact_evento Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evento</th>\n",
       "      <th>fecha</th>\n",
       "      <th>cliente_id</th>\n",
       "      <th>producto_id</th>\n",
       "      <th>cliente_conocido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>view</td>\n",
       "      <td>2015-06-02 05:50:14.164</td>\n",
       "      <td>1407399</td>\n",
       "      <td>248676</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>view</td>\n",
       "      <td>2015-06-02 05:02:17.106</td>\n",
       "      <td>1407399</td>\n",
       "      <td>367447</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>view</td>\n",
       "      <td>2015-06-02 05:12:03.240</td>\n",
       "      <td>1407399</td>\n",
       "      <td>443030</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>view</td>\n",
       "      <td>2015-06-02 05:34:51.897</td>\n",
       "      <td>1407399</td>\n",
       "      <td>439202</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>view</td>\n",
       "      <td>2015-06-02 05:16:02.373</td>\n",
       "      <td>1407399</td>\n",
       "      <td>10572</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   evento                   fecha  cliente_id  producto_id  cliente_conocido\n",
       "1    view 2015-06-02 05:50:14.164     1407399       248676             False\n",
       "4    view 2015-06-02 05:02:17.106     1407399       367447             False\n",
       "6    view 2015-06-02 05:12:03.240     1407399       443030             False\n",
       "7    view 2015-06-02 05:34:51.897     1407399       439202             False\n",
       "10   view 2015-06-02 05:16:02.373     1407399        10572             False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim_producto Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categoria_id</th>\n",
       "      <th>producto</th>\n",
       "      <th>marca_id</th>\n",
       "      <th>volumen</th>\n",
       "      <th>precio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356475</td>\n",
       "      <td>9</td>\n",
       "      <td>Crown Royal Honey</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>22.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15335</td>\n",
       "      <td>9</td>\n",
       "      <td>Crown Royal Regal Apple Mini</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>11.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>81345</td>\n",
       "      <td>9</td>\n",
       "      <td>Crown Royal Regal Apple</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>7.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150318</td>\n",
       "      <td>9</td>\n",
       "      <td>Crown Royal Xr Canadian Whiskey</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>98.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310791</td>\n",
       "      <td>9</td>\n",
       "      <td>Crown Royal Canadian Whisky Mini</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>11.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  categoria_id                          producto  marca_id  volumen  \\\n",
       "0  356475             9                 Crown Royal Honey         1      750   \n",
       "1   15335             9      Crown Royal Regal Apple Mini         1      300   \n",
       "2   81345             9           Crown Royal Regal Apple         1      200   \n",
       "3  150318             9   Crown Royal Xr Canadian Whiskey         1      750   \n",
       "4  310791             9  Crown Royal Canadian Whisky Mini         1      300   \n",
       "\n",
       "   precio  \n",
       "0   22.49  \n",
       "1   11.03  \n",
       "2    7.08  \n",
       "3   98.99  \n",
       "4   11.03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim_categoria Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SCOTCH WHISKIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>STRAIGHT BOURBON WHISKIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>BLENDED WHISKIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IMPORTED DRY GINS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>DECANTERS &amp; SPECIALTY PACKAGES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       categoria\n",
       "0   1                 SCOTCH WHISKIES\n",
       "1   2       STRAIGHT BOURBON WHISKIES\n",
       "2   3                BLENDED WHISKIES\n",
       "3   4               IMPORTED DRY GINS\n",
       "4   5  DECANTERS & SPECIALTY PACKAGES"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim_marca Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>marca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Diageo Americas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heaven Hill Brands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Sazerac Co., Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sage Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>MHW Ltd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               marca\n",
       "0   1     Diageo Americas\n",
       "1   2  Heaven Hill Brands\n",
       "2   3   Sazerac Co., Inc.\n",
       "3   4      Sage Beverages\n",
       "4   5             MHW Ltd"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dim_cliente Table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cliente</th>\n",
       "      <th>genero</th>\n",
       "      <th>edad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>599528</td>\n",
       "      <td>Samuel Ward</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121688</td>\n",
       "      <td>Willie Gonzales</td>\n",
       "      <td>Male</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552148</td>\n",
       "      <td>Betty Spencer</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102019</td>\n",
       "      <td>Beverly Jordan</td>\n",
       "      <td>Female</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189384</td>\n",
       "      <td>Cynthia Flores</td>\n",
       "      <td>Female</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          cliente  genero  edad\n",
       "0  599528      Samuel Ward    Male    36\n",
       "1  121688  Willie Gonzales    Male    53\n",
       "2  552148    Betty Spencer  Female    42\n",
       "3  102019   Beverly Jordan  Female    54\n",
       "4  189384   Cynthia Flores  Female    55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} Table:\")\n",
    "    display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_understanding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
