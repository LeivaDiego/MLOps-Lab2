{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd3d442",
   "metadata": {},
   "source": [
    "# Laboratorio 2: Data Understanding\n",
    "\n",
    "**Universidad del Valle de Guatemala**  \n",
    "**Facultad de Ingeniería**  \n",
    "**Departamento de Ciencias de la Computación**  \n",
    "**Machine Learning Operations** \n",
    "\n",
    "## Integrantes\n",
    "\n",
    "- Arturo Argueta - 21527 \n",
    "- Edwin de León - 22809 \n",
    "- Diego Leiva - 21752 \n",
    "- Pablo Orellana - 21970"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bd57d",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Tuple\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac025c5a",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ac29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Change to DEBUG for more detailed output\n",
    "    format=\"[%(levelname)s] - %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f3b86",
   "metadata": {},
   "source": [
    "## Lectura de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157e5bd",
   "metadata": {},
   "source": [
    "Durante el análisis exploratorio se encontró que hay conjuntos de datos con codificaciones diferentes a la típica utf-8, por lo que se necesita una lectura segura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1896e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_with_fallback(\n",
    "        path: Path, \n",
    "        encodings: Tuple[str, ...] = (\"utf-8\", \"latin1\", \"cp1252\"), \n",
    "        **pd_kwargs\n",
    "    ) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Attempts to read a CSV file with various encodings to find the correct one.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file.\n",
    "        encodings (tuple): Encodings to try.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, str]: The DataFrame read and the encoding used.\n",
    "    \"\"\"\n",
    "    last_error = None\n",
    "\n",
    "    # Iterating over encodings\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # Try reading the CSV with the current encoding\n",
    "            df = pd.read_csv(path, encoding=encoding, **pd_kwargs)\n",
    "            # If successful, return the DataFrame and the encoding used\n",
    "            return df, encoding\n",
    "\n",
    "        # If it fails, try the next encoding\n",
    "        except UnicodeDecodeError as e:\n",
    "            last_error = e\n",
    "\n",
    "        # If it fails, capture the error\n",
    "        except Exception as e:\n",
    "            # Other errors; keep track and keep trying in case it's just encoding\n",
    "            last_error = e\n",
    "\n",
    "    # Last \"tolerant\" attempt: utf-8 with replacement for bad characters\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            df = pd.read_csv(f, **pd_kwargs)\n",
    "            # If successful, return the DataFrame and the encoding used\n",
    "            return df, \"utf-8(errors=replace)\"\n",
    "    \n",
    "    # If not successful, keep track of the last error\n",
    "    except Exception:\n",
    "        raise last_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csvs(\n",
    "        folder: str,\n",
    "        pattern: str = \"*.csv\",\n",
    "        encodings: Tuple[str, ...] = (\"utf-8\", \"latin1\", \"cp1252\"),\n",
    "        **pd_kwargs\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load CSV files from a folder into a dictionary of DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        folder (str): The folder containing the CSV files.\n",
    "        pattern (str, optional): The glob pattern to match CSV files. Defaults to \"*.csv\".\n",
    "        encodings (Tuple[str, ...], optional): The encodings to try when reading CSV files. Defaults to (\"utf-8\", \"latin1\", \"cp1252\").\n",
    "        **pd_kwargs: Additional keyword arguments to pass to pandas read_csv.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: A dictionary mapping file names (without extensions) to DataFrames.\n",
    "    \"\"\"\n",
    "    # Get a list of all CSV files in the folder\n",
    "    files = sorted(Path(folder).glob(pattern))\n",
    "    dfs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # Read each CSV file into a DataFrame\n",
    "    for file in tqdm(files, desc=\"Reading CSVs\", unit=\"file\"):\n",
    "        df, encoding = read_csv_with_fallback(file, encodings=encodings, **pd_kwargs)\n",
    "        dfs[file.stem] = df # Store the DataFrame with the file name (without extension) as the key\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e083a",
   "metadata": {},
   "source": [
    "## Limpieza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ba273",
   "metadata": {},
   "source": [
    "### Manejo de duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a136d58",
   "metadata": {},
   "source": [
    "Durante el análisis exploratorio se encontró que los conjuntos de datos de clientes y eventos tienen registros completos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac69d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicates_handling(input_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle duplicates in a DataFrame.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame without duplicates.\n",
    "    \"\"\"\n",
    "    df_unique = input_dataframe.drop_duplicates()  # Delete duplicates\n",
    "\n",
    "    return df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087df1f",
   "metadata": {},
   "source": [
    "### Manejo de nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92e85c",
   "metadata": {},
   "source": [
    "Durante el análisis exploratorio se encontró que el conjunto de datos de  `clientes` tiene 281 registros nulos en cada columna (2.34%). en `eventos` solo la variable `transactionid` tiene 99.9% de valores nulos, y para `producto` las variables `categoria_id` y `marca_id` tienen 8.55% y 7.28% respectivamente y `precio` tiene un 0.05% de nulos.\n",
    "\n",
    "Para tratarlos se decide eliminar los resgistros con datos nulos para `clientes` y la variable `precio`, para las variables `categoria_id` y `marca_id` se decide mapearlos como desconocido u otros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17039506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_free_id(series: pd.Series) -> int:\n",
    "    \"\"\"\n",
    "    Get the next free (unused) ID from a series of existing IDs.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): Series of existing IDs.\n",
    "\n",
    "    Returns:\n",
    "        int: Next free ID.\n",
    "    \"\"\"\n",
    "    # Get the next free ID\n",
    "    if series.empty:\n",
    "        # If the series is empty, return 1 as the next ID\n",
    "        return 1\n",
    "    \n",
    "    return int(pd.to_numeric(series, errors=\"coerce\").max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ecbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_handling(dataframes: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Handle null values in the given DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        dataframes (Dict[str, pd.DataFrame]): The DataFrames to process.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: The processed DataFrames.\n",
    "    \"\"\"\n",
    "    out: Dict[str, pd.DataFrame] = {} # Initialize output dictionary\n",
    "\n",
    "    # Handle CATEGORIA\n",
    "    category_df = dataframes[\"categoria\"].copy() # create a copy of the categoria DataFrame\n",
    "    \n",
    "    # Check if 'Otro' category exists\n",
    "    if not (category_df[\"categoria\"] == \"Otro\").any():\n",
    "        # If not, create it\n",
    "        new_category_id = next_free_id(category_df[\"id\"])\n",
    "        # Append the new category with the next free ID and the name 'Otro'\n",
    "        category_df = pd.concat(\n",
    "            [category_df, pd.DataFrame([{\"id\": new_category_id, \"categoria\": \"Otro\"}])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    out[\"categoria\"] = category_df\n",
    "\n",
    "    # Handle MARCA\n",
    "    brand_df = dataframes[\"marca\"].copy() # create a copy of the marca DataFrame\n",
    "\n",
    "    # Check if 'Otro' brand exists\n",
    "    if not (brand_df[\"marca\"] == \"Otro\").any():\n",
    "        # If not, create it\n",
    "        new_brand_id = next_free_id(brand_df[\"id\"])\n",
    "        # Append the new brand with the next free ID and the name 'Otro'\n",
    "        brand_df = pd.concat(\n",
    "            [brand_df, pd.DataFrame([{\"id\": new_brand_id, \"marca\": \"Otro\"}])],\n",
    "            ignore_index=True\n",
    "        )\n",
    "    out[\"marca\"] = brand_df\n",
    "\n",
    "    # Handle CLIENTE\n",
    "    out[\"cliente\"] = dataframes[\"cliente\"].dropna(how=\"any\") # Drop rows with any null values\n",
    "\n",
    "    # Handle EVENTO\n",
    "    event_df = dataframes[\"evento\"].copy() # create a copy of the evento DataFrame\n",
    "    \n",
    "    # Check if 'transactionid' column exists\n",
    "    if \"transactionid\" in event_df.columns:\n",
    "        # Drop the 'transactionid' column\n",
    "        event_df = event_df.drop(columns=[\"transactionid\"])\n",
    "    out[\"evento\"] = event_df\n",
    "\n",
    "    # Handle PRODUCTO\n",
    "    product_df = dataframes[\"producto\"].copy()\n",
    "    product_df = product_df.dropna(subset=[\"precio\"]) # Drop rows with null values in 'precio'\n",
    "\n",
    "    # Check if 'categoria_id' column exists\n",
    "    if \"categoria_id\" in product_df.columns:\n",
    "        # Fill null values in 'categoria_id' with the ID of 'Otro' category\n",
    "        unknown_category_id = category_df.loc[category_df[\"categoria\"] == \"Otro\", \"id\"].iloc[0]\n",
    "        product_df[\"categoria_id\"] = product_df[\"categoria_id\"].fillna(unknown_category_id).astype(\"Int64\")\n",
    "\n",
    "    # Check if 'marca_id' column exists\n",
    "    if \"marca_id\" in product_df.columns:\n",
    "        # Fill null values in 'marca_id' with the ID of 'Otro' brand\n",
    "        unknown_brand_id = brand_df.loc[brand_df[\"marca\"] == \"Otro\", \"id\"].iloc[0]\n",
    "        product_df[\"marca_id\"] = product_df[\"marca_id\"].fillna(unknown_brand_id).astype(\"Int64\")\n",
    "    out[\"producto\"] = product_df\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f519edf",
   "metadata": {},
   "source": [
    "### Manejo de formatos de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(\n",
    "        input_dataframe: pd.DataFrame, \n",
    "        timestamp: str = \"timestamp\", \n",
    "        date: str = \"fecha\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert timestamp column to datetime.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame to process.\n",
    "        timestamp (str): Name of the timestamp column.\n",
    "        date (str): Name of the resulting date column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the converted date column.\n",
    "    \"\"\"\n",
    "    sample_val = input_dataframe[timestamp].iloc[0] # Get a sample value from the timestamp column\n",
    "    unit = 's' if sample_val < 1e11 else 'ms' # Determine the unit of the timestamp\n",
    "    input_dataframe[date] = pd.to_datetime(input_dataframe[timestamp], unit=unit) # Convert the timestamp column to datetime\n",
    "    return input_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_birthdate_and_age(\n",
    "        dataframes: Dict[str, pd.DataFrame], \n",
    "        key_data: str = \"cliente\", \n",
    "        birth_date: str = \"nacimiento\", \n",
    "        client_age: str = \"edad\"\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Clean birthdate and age fields in the specified DataFrame.\n",
    "    Handle parsing, future date correction, and age recalculation.\n",
    "\n",
    "    Args:\n",
    "        dataframes (Dict[str, pd.DataFrame]): Dictionary of DataFrames.\n",
    "        key_data (str): Key for the DataFrame to process.\n",
    "        birth_date (str): Name of the birthdate column.\n",
    "        client_age (str): Name of the age column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Updated dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "    df = dataframes[key_data].copy() # Create a copy of the DataFrame\n",
    "\n",
    "    # Parse date field to datetime format\n",
    "    df[birth_date] = pd.to_datetime(df[birth_date], format=\"%m/%d/%y\", errors=\"coerce\")\n",
    "\n",
    "    # Correct future years\n",
    "    future_date_mask = df[birth_date] > pd.Timestamp.today() # Identify future dates\n",
    "    df.loc[future_date_mask, birth_date] -= pd.DateOffset(years=100) # Subtract 100 years from future dates\n",
    "\n",
    "    # Recalculate age\n",
    "    today = pd.Timestamp.today() # Get the current date\n",
    "    df[client_age] = ((today - df[birth_date]).dt.days / 365.25).round() # Recalculate age\n",
    "\n",
    "    dataframes[key_data] = df # Update the DataFrame in the dictionary\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e5055",
   "metadata": {},
   "source": [
    "## Transformación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a02148",
   "metadata": {},
   "source": [
    "### Integridad referencial y enriquecimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b3d53",
   "metadata": {},
   "source": [
    "#### Filtro de productos válidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3032219",
   "metadata": {},
   "source": [
    "Se aplica un filtro obligatorio para conservar únicamente aquellos eventos cuyo `itemid` esté presente y referenciado en `producto.id`. De esta forma se eliminan todos los registros donde el producto del evento es desconocido, garantizando la integridad referencial con la tabla de productos y permitiendo calcular métricas consistentes de ventas, categorías y marcas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coerce_id_formats(\n",
    "        event_dataframe: pd.DataFrame,\n",
    "        product_dataframe: pd.DataFrame,\n",
    "        col_itemid: str = \"itemid\",\n",
    "        col_prod_id: str = \"id\"\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Coerce ID types for events and products DataFrames.\n",
    "\n",
    "    Args:\n",
    "        event_dataframe (pd.DataFrame): The events DataFrame.\n",
    "        product_dataframe (pd.DataFrame): The products DataFrame.\n",
    "        col_itemid (str): The column name for item IDs in events.\n",
    "        col_prod_id (str): The column name for product IDs in products.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: The coerced events and products DataFrames.\n",
    "    \"\"\"\n",
    "    # Create copies of the original DataFrames\n",
    "    event_df = event_dataframe.copy()\n",
    "    product_df = product_dataframe.copy()\n",
    "\n",
    "    # Normalize ID types to Int64 (allows NA) for clean comparisons\n",
    "    event_df[col_itemid] = pd.to_numeric(event_df[col_itemid], errors=\"coerce\").astype(\"Int64\")\n",
    "    product_df[col_prod_id] = pd.to_numeric(product_df[col_prod_id], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return event_df, product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d907fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_events_by_valid_product(\n",
    "        event_dataframe: pd.DataFrame,\n",
    "        product_dataframe: pd.DataFrame,\n",
    "        col_itemid: str = \"itemid\",\n",
    "        col_prod_id: str = \"id\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter events by valid product IDs.\n",
    "\n",
    "    Args:\n",
    "        event_dataframe (pd.DataFrame): DataFrame de eventos.\n",
    "        product_dataframe (pd.DataFrame): DataFrame de productos.\n",
    "        col_itemid (str): Nombre de la columna de itemid en el DataFrame de eventos.\n",
    "        col_prod_id (str): Nombre de la columna de id en el DataFrame de productos.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame de eventos filtrados.\n",
    "    \"\"\"\n",
    "    # Drop NA values from product IDs\n",
    "    prod_ids = product_dataframe[col_prod_id].dropna().unique()\n",
    "\n",
    "    # Generate mask for valid events\n",
    "    mask_valid = event_dataframe[col_itemid].isin(prod_ids)\n",
    "    valid_events = event_dataframe.loc[mask_valid].copy()\n",
    "\n",
    "    # Sanity check: all item IDs in valid events must be in product IDs\n",
    "    assert valid_events[col_itemid].dropna().isin(prod_ids).all(), \\\n",
    "        \"Quedaron itemid no válidos tras el filtro.\"\n",
    "\n",
    "    return valid_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b02f8fa",
   "metadata": {},
   "source": [
    "### Enriquecimiento de evento con datos de producto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f48e9e",
   "metadata": {},
   "source": [
    "Se integran los valores relacionados a la clave foránea de productos en la tabla de eventos, para enriquecer las visualizaciones posteriores con datos de los productos seleccionados, en lugar de usar el id del producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b44c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_product_features(\n",
    "        product_dataframe: pd.DataFrame,\n",
    "        category_dataframe: pd.DataFrame,\n",
    "        brand_dataframe: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Select and rename relevant features from product, category, and brand dataframes.\n",
    "\n",
    "    Args:\n",
    "        product_dataframe (pd.DataFrame): The product dataframe.\n",
    "        category_dataframe (pd.DataFrame): The category dataframe.\n",
    "        brand_dataframe (pd.DataFrame): The brand dataframe.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: The selected and renamed dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select relevant columns from product dataframe\n",
    "    selected_product_data = product_dataframe[\n",
    "        [\"id\", \"categoria_id\", \"nombre\", \"marca_id\", \"volumen\", \"precio\"]].copy()\n",
    "\n",
    "    # Rename product columns\n",
    "    selected_product_data = selected_product_data.rename(\n",
    "        columns={\"nombre\": \"producto\"})\n",
    "\n",
    "    # Rename category columns\n",
    "    selected_category_data = category_dataframe.rename(\n",
    "        columns={\"id\": \"categoria_id\", \"categoria\": \"categoria\"}).copy()\n",
    "\n",
    "    # Rename brand columns\n",
    "    selected_brand_data = brand_dataframe.rename(\n",
    "        columns={\"id\": \"marca_id\", \"marca\": \"marca\"}).copy()\n",
    "\n",
    "    return selected_product_data, selected_category_data, selected_brand_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a62702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_product(\n",
    "        event_dataframe: pd.DataFrame,\n",
    "        product_dataframe: pd.DataFrame,\n",
    "        col_itemid: str = \"itemid\",\n",
    "        col_prod_id: str = \"id\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich the event dataframe with product information,\n",
    "    by merging with the product dataframe.\n",
    "\n",
    "    Args:\n",
    "        event_dataframe (pd.DataFrame): The event dataframe.\n",
    "        product_dataframe (pd.DataFrame): The product dataframe.\n",
    "        col_itemid (str): The column name for item id in the event dataframe.\n",
    "        col_prod_id (str): The column name for product id in the product dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The enriched event dataframe.\n",
    "    \"\"\"\n",
    "    # Merge event and product dataframes where product id matches\n",
    "    merged_dataframe = event_dataframe.merge(\n",
    "        product_dataframe,\n",
    "        left_on=col_itemid,\n",
    "        right_on=col_prod_id,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged_dataframe = merged_dataframe.drop(columns=[col_prod_id])  # Drop duplicate product id\n",
    "    \n",
    "    return merged_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_category_and_brand(\n",
    "        event_dataframe: pd.DataFrame,\n",
    "        category_dataframe: pd.DataFrame,\n",
    "        brand_dataframe: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich event data with category and brand information.\n",
    "\n",
    "    Args:\n",
    "        event_dataframe (pd.DataFrame): The event DataFrame to enrich.\n",
    "        category_dataframe (pd.DataFrame): The category DataFrame.\n",
    "        brand_dataframe (pd.DataFrame): The brand DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The enriched event DataFrame.\n",
    "    \"\"\"\n",
    "    # Merge category information\n",
    "    df = event_dataframe.merge(category_dataframe, on=\"categoria_id\", how=\"left\")\n",
    "    # Merge brand information\n",
    "    df = df.merge(brand_dataframe, on=\"marca_id\", how=\"left\")\n",
    "    # Drop internal IDs that do not contribute\n",
    "    df = df.drop(columns=[\"categoria_id\", \"marca_id\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb5c13",
   "metadata": {},
   "source": [
    "### Enriquecimiento de evento con datos de cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b82f0",
   "metadata": {},
   "source": [
    "Se integran a la tabla de eventos las características relevantes de cada cliente a partir de la clave foránea `visitorid`, conservando únicamente los campos útiles para el análisis: nombre completo (combinación de nombre y apellido), género y edad previamente calculada. Para los registros sin coincidencia en la tabla de clientes se asigna la etiqueta `\"Anónimo\"` y se marca el indicador `cliente_conocido` como `False`, lo que permite diferenciar entre clientes identificados y no identificados. Este enfoque facilita segmentar métricas en el dashboard sin perder volumen de datos en los indicadores globales. \n",
    "\n",
    "**Nota**: a columna `ciudad` fue omitida debido a problemas de codificación (mojibake y mangled encoding) que no pudieron ser corregidos de forma confiable, evitando así mostrar valores ilegibles en el dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_client_features(client_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select relevant columns for client features.\n",
    "\n",
    "    Args:\n",
    "        client_dataframe (pd.DataFrame): The client DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with selected client features.\n",
    "    \"\"\"\n",
    "    # Select relevant columns for client features\n",
    "    cli_sel = client_dataframe[[\"id\", \"nombre\", \"apellido\", \"genero\", \"edad\"]].copy()\n",
    "    \n",
    "    return cli_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_client(\n",
    "        event_dataframe: pd.DataFrame,\n",
    "        client_dataframe: pd.DataFrame,\n",
    "        col_visitorid: str = \"visitorid\",\n",
    "        col_clientid: str = \"id\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enrich event data with client information by merging on visitor ID when possible.\n",
    "\n",
    "    Args:\n",
    "        event_dataframe (pd.DataFrame): The event DataFrame to enrich.\n",
    "        client_dataframe (pd.DataFrame): The client DataFrame to merge with.\n",
    "        col_visitorid (str): The column name for visitor ID in the event DataFrame.\n",
    "        col_clientid (str): The column name for client ID in the client DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The enriched event DataFrame.\n",
    "    \"\"\"\n",
    "    # Merge client information with event data\n",
    "    df = event_dataframe.merge(client_dataframe, left_on=col_visitorid, right_on=col_clientid, how=\"left\")\n",
    "\n",
    "    # Flag `cliente_conocido` to indicate known clients\n",
    "    df[\"cliente_conocido\"] = df[col_clientid].notna()\n",
    "\n",
    "    # Combine first and last name of client\n",
    "    df[\"cliente\"] = (df[\"nombre\"].fillna(\"\") + \" \" + df[\"apellido\"].fillna(\"\")).str.strip()\n",
    "    df.loc[~df[\"cliente_conocido\"], \"cliente\"] = \"Anónimo\" # Set anonymous for unknown clients\n",
    "\n",
    "    # Fill missing values of genre and age\n",
    "    df[\"genero\"] = df[\"genero\"].fillna(\"NA\")\n",
    "    df[\"edad\"] = df[\"edad\"]  # Fill missing values with NA\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=[\"nombre\", \"apellido\", col_clientid])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f11d6",
   "metadata": {},
   "source": [
    "### Selección de Variables de interés"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d84dcf",
   "metadata": {},
   "source": [
    "En esta etapa se depura el dataset resultante para conservar únicamente las columnas necesarias para el dashboard ejecutivo, eliminando campos internos o no relevantes para el análisis. Se renombra la columna `event` a `evento` para mayor claridad, y se mantiene la siguiente información: `evento`, `fecha`, `producto`, `volumen`, `precio`, `categoria`, `marca`, `cliente`, `genero`, `edad` y el indicador `cliente_conocido`. Este formato simplificado facilita la integración con Power BI y optimiza el rendimiento en la visualización, evitando la carga de datos innecesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a206bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(input_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select relevant features for the dashboard.\n",
    "\n",
    "    Args:\n",
    "        input_dataframe (pd.DataFrame): The input DataFrame to select features from.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with selected features.\n",
    "    \"\"\"\n",
    "    # Define final features for the dataframe\n",
    "    selected_columns = [\n",
    "        \"event\",\n",
    "        \"fecha\",\n",
    "        \"producto\",\n",
    "        \"volumen\",\n",
    "        \"precio\",\n",
    "        \"categoria\",\n",
    "        \"marca\",\n",
    "        \"cliente\",\n",
    "        \"genero\",\n",
    "        \"edad\",\n",
    "        \"cliente_conocido\"\n",
    "    ]\n",
    "\n",
    "    # Filter existing columns\n",
    "    cols_existentes = [c for c in selected_columns if c in input_dataframe.columns]\n",
    "    df_final = input_dataframe[cols_existentes].copy() # Make a copy\n",
    "    df_final = df_final.rename(columns={\"event\": \"evento\"}) # Rename event to evento\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22343a8",
   "metadata": {},
   "source": [
    "## Pipeline Integrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4debf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transform_pipeline(\n",
    "        raw_folder: str = \"data/raw\",\n",
    "        save_path: str = \"data/processed/boozemart_data.csv\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run the data transformation pipeline to process and clean the raw data.\n",
    "    Steps:\n",
    "    1) Load csv files\n",
    "    2) Handle duplicates\n",
    "    3) Handle null values\n",
    "    4) Convert timestamps to datetime\n",
    "    5) Process birth date and age of client\n",
    "    6) Coerce IDs and filter events by valid product relationships\n",
    "    7) Enrich event dataframe with product, brand and category information\n",
    "    8) Enrich event dataframe with client relevant data\n",
    "    9) Feature selection\n",
    "    10) Save the processed data into a csv file\n",
    "\n",
    "    Args:\n",
    "        raw_folder (str): Path to the folder containing raw data files.\n",
    "        save_path (str): Path to the folder where the processed data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed and cleaned data.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting data transformation pipeline...\")\n",
    "    \n",
    "    # 1) Load csv files\n",
    "    logger.info(\"Loading CSV files from %s\", raw_folder)\n",
    "    dfs = load_csvs(raw_folder, sep=\",\")\n",
    "\n",
    "    # 2) Handle duplicates\n",
    "    logger.info(\"Handling duplicates in dataframes...\")\n",
    "    dfs[\"cliente\"] = duplicates_handling(dfs[\"cliente\"])\n",
    "    dfs[\"evento\"]  = duplicates_handling(dfs[\"evento\"])\n",
    "\n",
    "    # 3) Handle null values\n",
    "    logger.info(\"Handling null values in dataframes...\")\n",
    "    dfs_limpio = nulls_handling(dfs)\n",
    "\n",
    "    # 4) Convert timestamps to datetime\n",
    "    logger.info(\"Converting timestamps to datetime...\")\n",
    "    dfs_limpio[\"evento\"] = convert_timestamp(dfs_limpio[\"evento\"], timestamp=\"timestamp\")\n",
    "\n",
    "    # 5) Process birth date and age of client\n",
    "    logger.info(\"Cleaning birthdate and age in client dataframe...\")\n",
    "    dfs_limpio = clean_birthdate_and_age(dfs_limpio, key_data=\"cliente\")\n",
    "\n",
    "    # 6) Coerce IDs and filter events by valid product relationships\n",
    "    logger.info(\"Coercing ID formats and filtering events by valid products...\")\n",
    "    evento_t, producto_t = coerce_id_formats(dfs_limpio[\"evento\"], dfs_limpio[\"producto\"])\n",
    "    valid_events = filter_events_by_valid_product(evento_t, producto_t)\n",
    "\n",
    "    # 7) Enrich event dataframe with product, brand and category information\n",
    "    logger.info(\"Enriching event dataframe with product, category, and brand information...\")\n",
    "    sel_prod, sel_cat, sel_brand = select_product_features(\n",
    "        dfs_limpio[\"producto\"], dfs_limpio[\"categoria\"], dfs_limpio[\"marca\"]\n",
    "    )\n",
    "    evento_prod = enrich_with_product(valid_events, sel_prod)\n",
    "    evento_full = enrich_with_category_and_brand(evento_prod, sel_cat, sel_brand)\n",
    "\n",
    "    # 8) Enrich event dataframe with client relevant data\n",
    "    logger.info(\"Enriching event dataframe with client information...\")\n",
    "    cli_sel = select_client_features(dfs_limpio[\"cliente\"])\n",
    "    evento_cli = enrich_with_client(evento_full, cli_sel)\n",
    "\n",
    "    # 9) Feature selection\n",
    "    logger.info(\"Selecting relevant features for the final dataframe...\")\n",
    "    final_df = feature_selection(evento_cli)\n",
    "\n",
    "    # 10) Save the processed data into a csv file\n",
    "    if save_path:\n",
    "        logger.info(\"Saving processed data to %s\", save_path)\n",
    "        final_df.to_csv(save_path, index=False, compression=\"gzip\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60d447",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boozemart_data = run_transform_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boozemart_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_understanding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
